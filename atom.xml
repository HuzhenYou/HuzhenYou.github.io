<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>九万里</title>
  
  <subtitle>虚怀若谷，大智若愚</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://huzhenyou.github.io/"/>
  <updated>2023-09-17T12:13:58.305Z</updated>
  <id>https://huzhenyou.github.io/</id>
  
  <author>
    <name>AngryBirds</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mmaction2之demo_spatiotemporal_det.py分析</title>
    <link href="https://huzhenyou.github.io//blog/2023/09/mmaction2.html"/>
    <id>https://huzhenyou.github.io//blog/2023/09/mmaction2.html</id>
    <published>2023-09-17T07:34:05.000Z</published>
    <updated>2023-09-17T12:13:58.305Z</updated>
    
    <content type="html"><![CDATA[<p>mmaction人体行为检测脚本demo/demo_spatiotemporal_det.py的分析。</p><a id="more"></a><hr><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>对人体行为检测demo脚本demo/demo_spatiotemporal_det.py进行分析。</p><h1 id="extract-all-frames"><a href="#extract-all-frames" class="headerlink" title="extract all frames"></a>extract all frames</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frame_paths, original_frames = frame_extract(args.video, </span><br><span class="line">out_dir=tmp_dir.name) <span class="comment"># extract all frames</span></span><br></pre></td></tr></table></figure><h1 id="sample-interval"><a href="#sample-interval" class="headerlink" title="sample interval"></a>sample interval</h1><ol><li>window_size = clip_len * frame_interval, here clip_len is the number of frames in the window,  frame_interval is the interval of frames in window.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">window_size = clip_len * frame_interval</span><br><span class="line"><span class="keyword">assert</span> clip_len % <span class="number">2</span> == <span class="number">0</span>, <span class="string">'We would like to have an even clip_len'</span></span><br><span class="line"><span class="comment"># Note that it's 1 based here</span></span><br><span class="line">timestamps = np.arange(window_size // <span class="number">2</span>, num_frame + <span class="number">1</span> - window_size // <span class="number">2</span>,</span><br><span class="line">   args.predict_stepsize)</span><br></pre></td></tr></table></figure><h1 id="human-detections"><a href="#human-detections" class="headerlink" title="human detections"></a>human detections</h1><ol><li>inference:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">human_detections, _ = detection_inference(args.det_config,</span><br><span class="line">                                              args.det_checkpoint,</span><br><span class="line">                                              center_frames,</span><br><span class="line">                                              args.det_score_thr,</span><br><span class="line">                                              args.det_cat_id, args.device)</span><br></pre></td></tr></table></figure><ol><li>restore to the original size:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(human_detections)):</span><br><span class="line">det = human_detections[i]</span><br><span class="line">det[:, <span class="number">0</span>:<span class="number">4</span>:<span class="number">2</span>] *= w_ratio</span><br><span class="line">det[:, <span class="number">1</span>:<span class="number">4</span>:<span class="number">2</span>] *= h_ratio</span><br><span class="line">human_detections[i] = torch.from_numpy(det[:, :<span class="number">4</span>]).to(args.device)</span><br></pre></td></tr></table></figure><h1 id="SpatioTemporal-Action-Detection"><a href="#SpatioTemporal-Action-Detection" class="headerlink" title="SpatioTemporal Action Detection"></a>SpatioTemporal Action Detection</h1><ol><li>get  all frames in a window according to a target frame(<code>timestamp</code>):</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_frame = timestamp - (clip_len // <span class="number">2</span> - <span class="number">1</span>) * frame_interval</span><br><span class="line">frame_inds = start_frame + np.arange(<span class="number">0</span>, window_size, frame_interval)</span><br><span class="line">frame_inds = list(frame_inds - <span class="number">1</span>)</span><br><span class="line">imgs = [frames[ind].astype(np.float32) <span class="keyword">for</span> ind <span class="keyword">in</span> frame_inds]</span><br></pre></td></tr></table></figure><ol><li>get the result of SpatioTemporal Action Detection:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.stack(imgs).transpose((<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))[np.newaxis]</span><br><span class="line">input_tensor = torch.from_numpy(input_array).to(args.device)</span><br><span class="line"></span><br><span class="line">datasample = ActionDataSample()</span><br><span class="line">datasample.proposals = InstanceData(bboxes=proposal)</span><br><span class="line">datasample.set_metainfo(dict(img_shape=(new_h, new_w)))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">result = model(input_tensor, [datasample], mode=<span class="string">'predict'</span>)</span><br><span class="line">scores = result[<span class="number">0</span>].pred_instances.scores</span><br><span class="line">prediction = []</span><br><span class="line"><span class="comment"># N proposals</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(proposal.shape[<span class="number">0</span>]):</span><br><span class="line">prediction.append([])</span><br><span class="line"><span class="comment"># Perform action score thr</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(scores.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_map:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(proposal.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">if</span> scores[j, i] &gt; args.action_score_thr:</span><br><span class="line">prediction[j].append((label_map[i], scores[j,i].item()))</span><br><span class="line">predictions.append(prediction)</span><br></pre></td></tr></table></figure><h1 id="Use-result"><a href="#Use-result" class="headerlink" title="Use result"></a>Use result</h1><ol><li>Show result in dense frames.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_timestamps</span><span class="params">(timestamps, n)</span>:</span></span><br><span class="line"><span class="string">"""Make it nx frames."""</span></span><br><span class="line">old_frame_interval = (timestamps[<span class="number">1</span>] - timestamps[<span class="number">0</span>])</span><br><span class="line">start = timestamps[<span class="number">0</span>] - old_frame_interval / n * (n - <span class="number">1</span>) / <span class="number">2</span> </span><br><span class="line">new_frame_inds = np.arange(</span><br><span class="line">len(timestamps) * n) * old_frame_interval / n + start</span><br><span class="line"><span class="keyword">return</span> new_frame_inds.astype(np.int64)</span><br></pre></td></tr></table></figure><p>here the meaning of <code>start = timestamps[0] - old_frame_interval / n * (n - 1) / 2</code>:</p><p><img src="/imgs/Pasted image 20230912092443.png" alt></p><h1 id="修复代码中的问题"><a href="#修复代码中的问题" class="headerlink" title="修复代码中的问题"></a>修复代码中的问题</h1><h2 id="加载大视频"><a href="#加载大视频" class="headerlink" title="加载大视频"></a>加载大视频</h2><p>问题描述: 当处理高清晰图像时，会在运行时崩溃掉。<br>原因如下：</p><ol><li>如我要处理的视频共510M, 但在 [[#extract all frames]] 这一步，把所有视频帧保存下来后，竟占用40多G，原因是视频进行了编码（类似于进行了压缩）。所以若遇到崩溃，首先看下，自己的磁盘空间是否足够。</li><li>更可气的是，每帧大小为(1440, 2560, 3)的图片，磁盘空间占用34G, 而内存却占用了309G, 原因是:<ol><li>内存 VS 硬盘( sys.getsizeof() VS os.path.getsize() ):  当加载一个文件到内存中时，用sys.getsizeof()获取变量大小，其包括文件数据及可能的其他python对象所占用的内存空间, 所以通常会远远大于文件在磁盘上的实际大小。</li></ol></li></ol><p>解决方案：对代码进行改造，分段进行识别。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mmaction人体行为检测脚本demo/demo_spatiotemporal_det.py的分析。&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>TriDet</title>
    <link href="https://huzhenyou.github.io//blog/2023/09/TriDet.html"/>
    <id>https://huzhenyou.github.io//blog/2023/09/TriDet.html</id>
    <published>2023-09-10T07:34:05.000Z</published>
    <updated>2023-09-10T09:15:22.205Z</updated>
    
    <content type="html"><![CDATA[<p>The detail of TriDet.</p><a id="more"></a><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ol><li>TriDet: Temporal Action Detection <strong>with</strong> Relative Boundary Modeling.</li><li><p>Temporal Action Detection(TAD)</p><ol><li>Detect all action boundaries and categories from an untrimmed video.<br><img src="/imgs/Pasted image 20230815192206.png" alt></li><li>The pipeline of TAD:<br><img src="/imgs/Pasted image 20230815192350.png" alt><ol><li>backbone:</li><li>use the pre-trained model in [[PoseEstimation_TAD#行为识别(Action Detection/Regnition)|Action Recognition Task]].</li><li>to get the feature map of each frame.</li></ol></li></ol></li><li><p>The core Focus of the author:</p><ol><li>get more accuracy boundary.</li><li>explore Transformer for TAD.</li></ol></li></ol><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h2><ol><li>two classes by the method of boundary dividing for TAD works using Transformer:<ol><li>[[#Segment-level prediction]]</li><li>[[#Instance-level prediction]]</li></ol></li></ol><h2 id="Segment-level-prediction"><a href="#Segment-level-prediction" class="headerlink" title="Segment-level prediction"></a>Segment-level prediction</h2><ol><li>based on extracted feature maps, get a clip, and simple, global expression(e.g., pooling), finally judge whether the clip is the target.</li><li>e.g. both of the below are two-stage network(like Faster-Rcnn, one stage generate lots of proposals, the second stage regresses and classifies proposals):<ol><li>[[BMNDetail|BMN]]</li><li>PGCN：<ol><li>use GCN to refine every proposal.</li></ol></li></ol></li><li>The method can’t be trained end-to-end.</li><li>End-to-end:<ol><li>TadTR</li><li>ReAct</li></ol></li></ol><h2 id="Instance-level-prediction"><a href="#Instance-level-prediction" class="headerlink" title="Instance-level prediction"></a>Instance-level prediction</h2><h3 id="Anchor-free-Detection"><a href="#Anchor-free-Detection" class="headerlink" title="Anchor-free Detection"></a>Anchor-free Detection</h3><h4 id="AFSD"><a href="#AFSD" class="headerlink" title="AFSD"></a>AFSD</h4><p><img src="/imgs/Pasted image 20230824200202.png" alt></p><ol><li>predict the distance to the start or end boundary.</li><li>then make the position pointed by most other position as the boundary.</li></ol><h4 id="ActionFormer"><a href="#ActionFormer" class="headerlink" title="ActionFormer"></a>ActionFormer</h4><p><img src="/imgs/Pasted image 20230824195951.png" alt></p><ol><li>Using slide-window to apply self-attention.</li><li>In 2022, the work in TAD improved obviously.</li><li>So TriDet carry out based on the work.</li></ol><h3 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h3><p><img src="/imgs/Pasted image 20230824201525.png" alt></p><h4 id="MLAD"><a href="#MLAD" class="headerlink" title="MLAD"></a>MLAD</h4><ol><li>Apply self-attention in the time level dimension and the classes level dimension.</li><li>Add the two attentions of  the two level dimensions to build the final feature map.</li></ol><h4 id="MS-TCT"><a href="#MS-TCT" class="headerlink" title="MS-TCT"></a>MS-TCT</h4><ol><li>Add a CNN module after a traditional self-attention module.</li><li>Add residual connections.</li></ol><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol><li>For the above  two works,  they both pay attention to modifying  the self-attention.</li><li>So, it indicates that the original self-attention cannot be applied to TAD.</li></ol><h2 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><h3 id="Segment-level-prediction-1"><a href="#Segment-level-prediction-1" class="headerlink" title="Segment-level prediction"></a>Segment-level prediction</h3><ol><li>Contain global representation of segments.</li><li>Larger receptive field.</li><li>More information.</li><li>Detailed Information at each instant is discarded.</li><li>Highly depend on the accuracy of segments.<br><img src="/imgs/Pasted image 20230829191215.png" alt></li></ol><h3 id="Instant-level-prediction"><a href="#Instant-level-prediction" class="headerlink" title="Instant-level prediction"></a>Instant-level prediction</h3><ol><li>Contain detailed representation of instants.</li><li>Smaller receptive field.</li><li>The requirement for feature distinguishability is high. (use the strong backbone to extract features.)</li><li>The degree of response varies greatly with different videos.</li></ol><h1 id="Motivation-of-Trident-head"><a href="#Motivation-of-Trident-head" class="headerlink" title="Motivation of Trident-head"></a>Motivation of Trident-head</h1><ol><li>Consider both instant-level and segment-level feature.</li><li>Set it as the segment-level feature that the predicted frame with the  fixed number of adjacent frames.</li><li>Set the segment-level feature as instant-level feature?</li></ol><h1 id="Trident-head"><a href="#Trident-head" class="headerlink" title="Trident-head"></a>Trident-head</h1><p><img src="/imgs/Pasted image 20230904090904.png" alt></p><ol><li>Three branch:<ol><li><span style="color:#FF8C00;">Start Boundary </span>  and <span style="color:#FF8C00;">End Boundary </span> would extract the segment-level feature.</li><li><span style="color:#FF8C00;">Center Offset </span> would extract instance-level feature.</li></ol></li><li>E.G., Predicted <span style="color:Purple;">Start Boundary</span>  is decided by <span style="color:#FF8C00;">Start Boundary </span> and  <span style="color:#FF8C00;">Center Offset </span>:<br><img src="/imgs/Pasted image 20230904182444.png" alt></li><li>Expectation is decided by B:<ol><li>if B is too small, we can’t find the more far boundary.</li><li>if B is too big, the difficulty of  learning and convergence of training is more great, so that the predicted result is not accuracy.</li></ol></li><li>Combined with FPN:<br><img src="/imgs/Pasted image 20230904184206.png" alt><ol><li>In the different level layers, the fixed number of B is set to product different Bs, so it can have small and big Bs simultaneously.</li><li>While finally outputting, the predicted results in different layers times corresponding scale ratio to get real position.</li></ol></li></ol><h1 id="The-second-question-Attention-in-Temporal-Dimension"><a href="#The-second-question-Attention-in-Temporal-Dimension" class="headerlink" title="The second question: Attention in Temporal Dimension."></a>The second question: Attention in Temporal Dimension.</h1><ol><li>Many methods require complex attention mechanisms to make the network work.</li><li>The success of the previous transformer-based layers(in TAD) primarily relies on their macro-architecture, rather than the self-attention mechanism.<br><img src="/imgs/Pasted image 20230904193650.png" alt></li><li>Above, when 1D-Conv take the place of Self-attention, the Avg map only drops by 1.9, but  when CNN baseline takes the place of Transformer baseline, the Avg map drops very much,  which indicates that Transformer is effective depending to its structure not Self-attention.<br><img src="/imgs/4. ![[The Rank Loss Problem of Self Attention" alt><br>In TAD, making the features same is disastrous, because we need to distinguish a position is the action or not.</li><li><p>Pure LayerNorm will normalize the features <script type="math/tex">x\in R^n</script> to a modulus <script type="math/tex">\sqrt{n}</script> :</p><script type="math/tex; mode=display">x' = LayerNorm(x)</script><script type="math/tex; mode=display">x'_i = \frac{x_i-mean(x)}{\frac{1}{n} {\textstyle \sum_{n}^{}(x_i-mean(x))^2}}</script><script type="math/tex; mode=display">\left \| x' \right \| ^2_2 = {\textstyle \sum_{n}^{}x'^2_i}=n</script></li><li><p>The Evidence on HACS:<br>we consider the cosine similarity:<br><img src="/imgs/Pasted image 20230904203951.png" alt></p><ol><li>here SA is Self-Attention, SGP is proposed by the author, BackB is the backbone network to extract the feature.</li><li>here the value is the average of the cosine similarity of every feature and average feature in the same layer ?</li></ol></li><li>Consider the Self Attention:<script type="math/tex; mode=display">V'=WV</script><script type="math/tex; mode=display">W=Softmax(\frac{QK^T}{\sqrt{d}})</script>W is non-negative and the sum of each row is 1, thus the <script type="math/tex">V'</script> are [[ConceptAI#convex combination|convex combination]] for the input <script type="math/tex">V</script>.<br>But the value in Convolution kernel can be negative, and the sum can be not 1.</li></ol><h1 id="The-author’s-Solution-The-SGP-layer"><a href="#The-author’s-Solution-The-SGP-layer" class="headerlink" title="The author’s Solution: The SGP layer"></a>The author’s Solution: The SGP layer</h1><ol><li>increase the discrimination of feature.</li><li><p>capture temporal information with different scales of receptive fields.</p><script type="math/tex; mode=display">f_{SGP}=\Phi (x)FC(x)+\psi(x)(Conv_w(x)+Conv_{kw}(x))+x$$,$$\Phi(x)=ReLU(FC(AvgPool(x)))$$,$$\psi(x)=Conv_w(x)</script><p>Window-level: make the network extract features in different scales adaptively.</p><p>In detail:</p><ol><li>the author uses the depth-wise convolution to reduce the computation of the network.</li><li>add a additional residual connection.</li></ol></li></ol><p><img src="/imgs/Pasted image 20230904211744.png" alt></p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>The performance is so good: the accuracy is higher, the speed is faster.</p><p><img src="/imgs/Pasted image 20230905084929.png" alt></p><p><img src="/imgs/Pasted image 20230905085657.png" alt></p><h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ol><li><a href="https://www.bilibili.com/video/BV12M4y117GZ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2c23be48ba22c91130ce4868020ab598" target="_blank" rel="noopener">https://www.bilibili.com/video/BV12M4y117GZ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2c23be48ba22c91130ce4868020ab598</a>  (‘4.10)</li><li>Paper: <a href="https://arxiv.org/abs/2303.07347" target="_blank" rel="noopener">https://arxiv.org/abs/2303.07347</a></li><li>Code: <a href="https://github.com/dingfengshi/TriDet" target="_blank" rel="noopener">https://github.com/dingfengshi/TriDet</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The detail of TriDet.&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="CVPR2023" scheme="https://huzhenyou.github.io/tags/CVPR2023/"/>
    
      <category term="TemporalActionDetection" scheme="https://huzhenyou.github.io/tags/TemporalActionDetection/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>BMN详解</title>
    <link href="https://huzhenyou.github.io//blog/2023/08/BMN%E8%AF%A6%E8%A7%A3.html"/>
    <id>https://huzhenyou.github.io//blog/2023/08/BMN详解.html</id>
    <published>2023-08-13T07:34:05.000Z</published>
    <updated>2023-09-10T09:02:13.160Z</updated>
    
    <content type="html"><![CDATA[<p>BMN(Boundary-Matching Network) 详解。</p><a id="more"></a><hr><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>百度，ActivityNet Challenge 2019 冠军模型：BMN: Boundary-Matching Network for Temporal Action Proposal Generation。</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><ol><li>Unlike temporal action detection task, in the work categories of action instances are not taken into account in proposal generation task.</li><li>The temporal annotation: <script type="math/tex">\Psi_g=\left \{ \varphi_n=(t_{s,n}, t_{e,n})  \right \} ^{N_g}_{n=1}</script>, here <script type="math/tex">N_g</script> is  the amount of ground-truth action instances.</li><li>During inference, proposal generation method should generate proposals <script type="math/tex">\Psi_p</script> which cover  <script type="math/tex">\Psi_g</script> precisely and exhaustively.</li></ol><h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p><img src="/imgs/Pasted image 20230807194737.png" alt><br><img src="/imgs/Pasted image 20230807194836.png" alt></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li>Feature Extraction: 使用双流网络（光流+RGB），获得feature map.</li><li>Base Module： 1x1卷积（时序卷积）。</li><li>Temporal Evaluation Module(TEM)： 1x1卷积（时序卷积），获得开始、结束点的概率序列。</li><li>Proposal Evaluation Module(PEM)：<ol><li>[[PapersRead#BMN#BM layer|BM layer]]</li><li>通过conv3d, conv2d 得到置信度图。</li></ol></li><li>生成结果：<ol><li>把两条边界概率序列中大于<script type="math/tex">极大值\times  \frac{1}{2}</script> 或是峰值（极大值）的都看作开始或结束边界.</li><li>$n^2$复杂度两两组合，获得一系列proposals:</li><li>根据置信度图获得各个proposals的置信度。<ol><li>the proposal denoted: <script type="math/tex">φ = (t_s, t_e, p^s_{ts} , p^e_{te} , p_{cc}, p_{cr})</script><br>  where <script type="math/tex">p^s_{ts},  p^e_{te}</script> are starting and ending probabilities.  <script type="math/tex">p_{cc}, p_{cr}</script> are classification confidence and regression confidence score.</li><li>get final score: <script type="math/tex">p_f = p^s_{ts} · p^e_{te} · \sqrt{p_{cc}· p_{cr})}</script></li></ol></li><li>利用Soft-NMS去冗余。</li></ol></li></ol><h2 id="置信度图："><a href="#置信度图：" class="headerlink" title="置信度图："></a>置信度图：</h2><p><img src="/imgs/Pasted image 20230807202541.png" alt></p><ol><li><script type="math/tex">M_C\in R^{D×T}</script>.</li><li>由开始点和长度决定结束点，从而确定一个proposal. 所以上图对应所有任意视频段的置信度。</li><li>duration dim: proposal长度.</li><li>starting dim: 开始点位置。</li><li>同一行对应的proposals对应相同的长度。同一列队对应的proposals拥有相同的开始点。同一负对角线对应的proposals拥有相同的结束边界。右下角部分proposals超出视频范围，无意义。</li></ol><h2 id="BM-layer"><a href="#BM-layer" class="headerlink" title="BM layer"></a>BM layer</h2><ol><li>The goal: uniformly sample N points in <script type="math/tex">S_{F} ∈ R^{C×T}</script>  between starting boundary <script type="math/tex">t_{s}</script> and ending boundary <script type="math/tex">t_{e}</script> of each proposal <script type="math/tex">φ_{i,j}</script>, and get proposal feature <script type="math/tex">m^f_{i,j} ∈ R^{C×N}</script> with rich context (actually sampling in [<script type="math/tex">t_S-0.25d, t_e+0.25d</script>]).<br> <img src="/imgs/Pasted image 20230810081026.png" alt></li><li>two problems: <ol><li>how to sample feature in non-integer point:<br> ![[Pasted image 20230808205131.png|375]]</li><li>how to sample feature for all proposals simultaneously:<ol><li>expanding <script type="math/tex">w_{i,j} ∈ R^{N ×T}</script>  to  <script type="math/tex">W ∈ R^{N ×T ×D×T}</script> for all proposals in BM confidence map.</li><li>get <script type="math/tex">M_F ∈ R^{C×N×D×T}</script> by using dot product:   <script type="math/tex">S_{F} ∈ R^{C×T}</script> and  <script type="math/tex">W^T ∈ R^{T×N×D×T}</script>. （<em><script type="math/tex">W</script> can be pre-generated because it’s the same for different videos, the inference speed of BM layer is very fast.</em> Is T is the same for the different videos? Ans: [[#BMN#Base module|Base module]] and [[#BMN#Training of BMN#Training Data Construction|Training Data Construction]]. TODO: review code）</li></ol></li></ol></li></ol><h2 id="Base-module"><a href="#Base-module" class="headerlink" title="Base module"></a>Base module</h2><ol><li><em>adopt a long observation window with length <script type="math/tex">l_ω</script> to truncate the untrimmed feature sequence with length <script type="math/tex">l_f</script> .</em></li><li>So here <script type="math/tex">l_w</script> is <script type="math/tex">T</script> in <script type="math/tex">S_{F} ∈ R^{C×T}</script>.</li></ol><h2 id="Proposal-Evaluation-Module-PEM"><a href="#Proposal-Evaluation-Module-PEM" class="headerlink" title="Proposal Evaluation Module(PEM)"></a>Proposal Evaluation Module(PEM)</h2><ol><li>Final generate: <script type="math/tex">M_C\in R^{D×T}</script>, but there are two predicted <script type="math/tex">M_C</script>: <script type="math/tex">M_{CC}</script>, <script type="math/tex">M_{CR}</script>, being trained using binary classification and regression loss function separately. TODO: review code.</li></ol><h2 id="Training-of-BMN"><a href="#Training-of-BMN" class="headerlink" title="Training of BMN"></a>Training of BMN</h2><h3 id="TEM-vs-PEM"><a href="#TEM-vs-PEM" class="headerlink" title="TEM vs PEM:"></a>TEM vs PEM:</h3><ol><li>TEM: learns local boundary.</li><li>PEM: pattern global proposal context.</li></ol><h3 id="Training-Data-Construction"><a href="#Training-Data-Construction" class="headerlink" title="Training Data Construction:"></a>Training Data Construction:</h3><ol><li>firstly, extract all feature sequence F with length.</li><li>get many observation windows with length <script type="math/tex">l_w</script> with 50% overlap.</li><li>here every window contains at least one ground-truth action instance.</li></ol><h3 id="Label-Assignment"><a href="#Label-Assignment" class="headerlink" title="Label Assignment"></a>Label Assignment</h3><h4 id="TEM"><a href="#TEM" class="headerlink" title="TEM"></a>TEM</h4><ol><li>denote its starting and ending regions as <script type="math/tex">r_S = [t_s − d_g /10, t_s +d_g/10]</script> and $r_E =[t_e−d_g/10,t_e+d_g/10]$separately.</li><li>denote its local region as <script type="math/tex">r_{t_n} = [t_n −d_f /2, t_n +d_f /2]</script>, where <script type="math/tex">d_f = t_n −t_{n−1}</script> is the temporal interval between two locations. </li><li>Then calculate overlap ratio IoR of <script type="math/tex">r_{t_n}</script> with <script type="math/tex">r_S</script> and <script type="math/tex">r_E</script> separately, and denote maximum IoR as <script type="math/tex">g^s_{t_n}</script> and  <script type="math/tex">g^e_{t_n}</script> separately.</li><li><em>here IoR is defined as the overlap ratio with ground-truth proportional to the duration of this region.</em> TODO: code review.</li><li>Thus generate <script type="math/tex">G_{S,ω}=\{g^s_{t_n}\}^{l_w}_{n=1}</script> and <script type="math/tex">G_{E,ω}=\{g^e_{t_n}\}^{l_w}_{n=1}</script>  as label of TEM.</li></ol><h4 id="PEM"><a href="#PEM" class="headerlink" title="PEM"></a>PEM</h4><ol><li>Purpose: BM label map <script type="math/tex">G_C ∈ R^{D×T}</script>.</li><li>For a proposal <script type="math/tex">φ_{i,j}=(t_s=t_j, t_e=t_j+t_i)</script> , calculate its IoU with all <script type="math/tex">φ_g</script> in <script type="math/tex">Ψ_ω</script>, and denote the maximum IoU as <script type="math/tex">g^c_{i,j}</script> . Thus we can generate <script type="math/tex">G_C=\{g^c_{i,j}\}^{D,l_ω}_{i,j=1}</script>  as label of PEM.</li></ol><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Loss-of-TEM"><a href="#Loss-of-TEM" class="headerlink" title="Loss of TEM"></a>Loss of TEM</h4><ol><li>adopt weighted binary logistic regression loss function <script type="math/tex">L_{bl}</script>,  to get the sum of starting and ending losses:<script type="math/tex; mode=display">L_{TEM} =L_{bl}(P_S,G_S)+L_{bl}(P_E,G_E)</script></li><li><script type="math/tex; mode=display">L_{bl}(P,G): \frac{1}{l_w}\sum_{i=1}^{l_w}(a^+·b_i·log(p_i)+a^-·(1-b_i)·log(1-p_i))</script> where <script type="math/tex">b_i = sign(g_i − θ)</script> is a two-value function used to convert <script type="math/tex">g_i</script> from [0, 1] to {0, 1}0, 1} based on overlap threshold$θ = 0.5$. Denoting <script type="math/tex">l^+=\sum b_i</script>  and <script type="math/tex">l^− = l_ω −l^+</script>, the weighted terms are <script type="math/tex">α^+ = \frac{l_w}{l^+}</script> and <script type="math/tex">α^- = \frac{l_w}{l^-}</script>.</li></ol><h4 id="Loss-of-PEM"><a href="#Loss-of-PEM" class="headerlink" title="Loss of PEM"></a>Loss of PEM</h4><ol><li>Define:<script type="math/tex; mode=display">L_{PEM} =L_C(M_{CC},G_C)+λ·L_R(M_{CR},G_C)</script><ol><li>here  <script type="math/tex">L_{bl}</script>  for <script type="math/tex">L_C</script>  ,  L2 loss for <script type="math/tex">L_R</script> .  <script type="math/tex">λ = 10</script> . </li><li>to balance the ratio between positive and negative samples in <script type="math/tex">L_R</script> , take all points with <script type="math/tex">g^C_{i,j}>0.6</script>  as positive, and randomly sample  <script type="math/tex">g^C_{i,j}<0.2</script>  as negative, ensure 1:1 for positive: negative.</li></ol></li></ol><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><script type="math/tex; mode=display">L=L_{LEM} +λ_1 ·L_{GEM} +λ_2 ·L_2(Θ)</script><p>where <script type="math/tex">L_2(Θ)</script> is L2 regularization term,  <script type="math/tex">λ_1</script>, <script type="math/tex">λ_2</script> are set to 1, 0.000 to ensure different modules are trained evenly.</p><p>Refs: </p><ol><li><a href="https://zhuanlan.zhihu.com/p/337432552" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/337432552</a></li><li><a href="https://arxiv.org/pdf/1907.09702.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.09702.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BMN(Boundary-Matching Network) 详解。&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
      <category term="ProposalGeneration" scheme="https://huzhenyou.github.io/tags/ProposalGeneration/"/>
    
      <category term="ActivityNetChallenge2019" scheme="https://huzhenyou.github.io/tags/ActivityNetChallenge2019/"/>
    
  </entry>
  
</feed>
