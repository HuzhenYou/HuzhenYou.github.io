<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>九万里</title>
  
  <subtitle>虚怀若谷，大智若愚</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://huzhenyou.github.io/"/>
  <updated>2024-01-10T02:59:31.235Z</updated>
  <id>https://huzhenyou.github.io/</id>
  
  <author>
    <name>AngryBirds</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大模型调研</title>
    <link href="https://huzhenyou.github.io//blog/2024/01/LargeModelSurvey.html"/>
    <id>https://huzhenyou.github.io//blog/2024/01/LargeModelSurvey.html</id>
    <published>2024-01-10T07:34:05.000Z</published>
    <updated>2024-01-10T02:59:31.235Z</updated>
    
    <content type="html"><![CDATA[<p>对当前（2023.12.30）大模型进行调研。</p><a id="more"></a><hr><h1 id="Large-Model"><a href="#Large-Model" class="headerlink" title="Large Model"></a>Large Model</h1><h2 id="行业发展"><a href="#行业发展" class="headerlink" title="行业发展"></a>行业发展</h2><p><img src="/imgs/Pasted image 20231214180048.png" alt></p><h3 id="国内"><a href="#国内" class="headerlink" title="国内"></a>国内</h3><p><img src="/imgs/Pasted image 20231214175942.png" alt></p><h2 id="开源模型"><a href="#开源模型" class="headerlink" title="开源模型"></a>开源模型</h2><ol><li>2022年8月，Stable Diffusion问世，让DALL·E的神秘光环不再遥不可及。</li><li>2023年2月，Meta的Llama。</li><li>2023年12月21日，智源研究院发布了新一代多模态基础模型 Emu2（开源版Gemini）。（智源重大科研项目均为青年人才主导，来自清华、北大、Facebook人工智能实验室、Mila实验、微软亚洲研究院、百度、华为、快手等海内外顶尖研究机和知名科技与互联网企业，于2018年11月正式揭牌。）</li></ol><h2 id="Emu2"><a href="#Emu2" class="headerlink" title="Emu2"></a>Emu2</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><ol><li>被誉为 开源版Gemini。</li><li>Emu2在少样本多模态理解任务上大幅超越Flamingo-80B、IDEFICS-80B等主流多模态预训练大模型，在包括VQAv2、OKVQA、MSVD、MM-Vet、TouchStone在内的多项少样本理解、视觉问答、主体驱动图像生成等任务上取得最优性能。<img src="/imgs/Pasted image 20231227155625.png" alt>   <img src="/imgs/Pasted image 20231227170528.png" alt></li><li>Emu2是目前最大的开源生成式多模态模型，基于Emu2微调的Emu2-Chat和Emu2-Gen模型分别是目前开源的性能最强的视觉理解模型和能力最广的视觉生成模型：<ol><li>Emu2-Chat可以精准理解图文指令，实现更好的信息感知、意图理解和决策规划。</li><li>Emu2-Gen可以接受图像、文本、位置交错的序列作为输入，实现灵活、可控、高质量的图像和视频生成。</li></ol></li><li>Zero-shot: 在零样本的DreamBench主体驱动图像生成测试上，较此前方法取得显著提升，例如比Salesforce的BLIP-Diffusion的CLIP-I分数高7.1%, 比微软的Kosmos-G的DINO分数高7.2%。</li><li>统一的生成式预训练：使用统一的自回归建模方式，根据当前已生成的 token 预测下一个视觉或文本token：<img src="/imgs/Pasted image 20231227165028.png" alt></li><li>相比Emu1，Emu2使用了更简单的建模框架、训练了更好的从特征重建原图的解码器、并把模型规模化到37B参数。</li></ol><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>基于Emu2微调的Emu2-Chat和Emu2-Gen模型分别是目前开源的性能最强的视觉理解模型和能力最广的视觉生成模型。</p><h4 id="Emu2-Chat"><a href="#Emu2-Chat" class="headerlink" title="Emu2-Chat"></a>Emu2-Chat</h4><p>推理图像中的要素、读指示牌提供引导、按要求提取和估计指定属性、回答简单的专业学科问题等。<br><img src="/imgs/Pasted image 20231227165420.png" alt><br><img src="/imgs/Pasted image 20231227165503.png" alt><br><img src="/imgs/Pasted image 20231227165629.png" alt></p><h4 id="Emu2-Gen"><a href="#Emu2-Gen" class="headerlink" title="Emu2-Gen"></a>Emu2-Gen</h4><p><img src="/imgs/Pasted image 20231227161646.png" alt></p><h4 id="Emu2基于任意prompt序列的视频生成"><a href="#Emu2基于任意prompt序列的视频生成" class="headerlink" title="Emu2基于任意prompt序列的视频生成"></a>Emu2基于任意prompt序列的视频生成</h4><p><img src="/imgs/640.gif" alt></p><p>Refs:</p><blockquote><p>项目：<a href="https://baaivision.github.io/emu2/" target="_blank" rel="noopener">Generative Multimodal Models are In-Context Learners</a><br>模型：<a href="https://huggingface.co/BAAI/Emu2" target="_blank" rel="noopener">BAAI/Emu2 · Hugging Face</a><br>代码：<a href="https://github.com/baaivision/Emu/tree/main/Emu2" target="_blank" rel="noopener">Emu/Emu2 at main · baaivision/Emu · GitHub</a><br>Demo：<a href="https://huggingface.co/spaces/BAAI/Emu2" target="_blank" rel="noopener">Emu2 - a Hugging Face Space by BAAI</a><br>论文：<a href="https://arxiv.org/abs/2312.13286" target="_blank" rel="noopener">[2312.13286] Generative Multimodal Models are In-Context Learners</a><br><a href="https://mp.weixin.qq.com/s/2uF1UM3Kraeq8nHltX9pCA" target="_blank" rel="noopener">全球最强「开源版Gemini」诞生！全能多模态模型Emu2登热榜，多项任务刷新SOTA</a></p></blockquote><h2 id="百度文心一言"><a href="#百度文心一言" class="headerlink" title="百度文心一言"></a>百度文心一言</h2><ol><li>开放了类似GPTs的AI智能体平台：灵境矩阵。</li><li>灵境矩阵：打通了百度搜索、文心一言插件商城等场景。</li><li>OpenAI科学家Andrej Karpathy表示：AI智能体代表了AI一个未来。</li><li>为什么需要智能体：目前的大模型过于通用化了，在更多时候，我们需要的是具有特殊性的、能完成指定任务的AI。就好比一个大学毕业生还难以胜任专业性较强的工作，还需要经过专业知识和技能培训才能上岗。</li></ol><p>Refs:</p><blockquote><p><a href="https://plugin.baidu.com/center" target="_blank" rel="noopener">灵境矩阵 | 想象即现实</a><br><a href="https://mp.weixin.qq.com/s/38dGQf6uP6R8bpe_rbnp9Q" target="_blank" rel="noopener">文心4.0加持、0代码开发，自带流量的智能体平台来了！</a></p></blockquote><h2 id="Bard"><a href="#Bard" class="headerlink" title="Bard"></a>Bard</h2><ol><li>已经集成微调版的Gemini Pro.</li><li>调用Gemini Pro，目前只能支持英文（2023.12.19）.</li><li>可以用插件搜索，如：help search youtube for  XIjinping</li><li>支持的插件有：<br><img src="/imgs/Pasted image 20231219172659.png" alt></li><li>其知识会及时更新：<br><img src="/imgs/Pasted image 20231226171351.png" alt></li></ol><h2 id="Gemini"><a href="#Gemini" class="headerlink" title="Gemini"></a>Gemini</h2><h3 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h3><p>12 月 7 日，谷歌发布原生多模态大模型 Gemini。几乎全方位超越 GPT-4。<br>特点：</p><ol><li>括三种量级：能力最强的 Gemini Ultra，适用于多任务的 Gemini Pro 以及适用于特定任务和端侧的 Gemini Nano，实现了更为高级的推理、规划、理解等能力。</li><li>采用高效的attention机制，如multi-query attention。</li><li>支持32k的上下文长度。</li><li>原生多模态，30 项学术基准取得最优。Gemini 是一个原生的多模态大模型，是将文本、代 码、图片、视频、语音合在一起放进模型里训练而来的，因此能实现 更均衡的多模态输出及任意模型切换。Gemini Ultra 首次在 MMLU（大 规模多任务语言理解）测评上超过人类专家，在 32 个多模态基准中 取得 30 个 SOTA（当前最优效果）</li><li>强大的图像/视频等多模态推理能力。空间逻辑推理能力。时间线推理能力。图文理解能力。交错图文生成 能力 等。</li></ol><p><img src="/imgs/Screenshot 2023-12-19 at 8.02.47 PM.png" alt></p><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><h4 id="Intro-2"><a href="#Intro-2" class="headerlink" title="Intro"></a>Intro</h4><ol><li>三种方式：<ol><li>freeform prompt.</li><li>structured prompt.</li><li>chat prompt。</li></ol></li><li>通过get code，可快速得到微调后的模型调用方式，如： [[#^cd527c]]</li><li>可工作模型参数：Temperature 控制模型输出的灵活度。</li><li>提供两种方式：<ol><li>纯对话：Gemini Pro</li><li>图片解析：Gemini Pro Vision. 但不支持生图（2023.12.20）.</li></ol></li></ol><h4 id="freeform-prompt"><a href="#freeform-prompt" class="headerlink" title="freeform prompt"></a>freeform prompt</h4><ol><li>可定义 文字段 作为变量，输出多个答案：<br><img src="/imgs/Pasted image 20231220100343.png" alt></li></ol><h4 id="structured-prompt"><a href="#structured-prompt" class="headerlink" title="structured prompt"></a>structured prompt</h4><ol><li>构建prompt exmples, 进行few-shot.</li><li>例如：<br><img src="/imgs/Pasted image 20231220091049.png" alt><img src="/imgs/Pasted image 20231220090929.png" alt> ^cd527c</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">At the command line, only need to run once to install the package via pip:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ pip install google-generativeai</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> google.generativeai <span class="keyword">as</span> genai</span><br><span class="line"></span><br><span class="line">genai.configure(api_key=<span class="string">"YOUR_API_KEY"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Set up the model</span></span><br><span class="line">generation_config = &#123;</span><br><span class="line">  <span class="string">"temperature"</span>: <span class="number">0.65</span>,</span><br><span class="line">  <span class="string">"top_p"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="string">"top_k"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="string">"max_output_tokens"</span>: <span class="number">2048</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">safety_settings = [</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_HARASSMENT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_HATE_SPEECH"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_SEXUALLY_EXPLICIT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_DANGEROUS_CONTENT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model = genai.GenerativeModel(model_name=<span class="string">"gemini-pro"</span>,</span><br><span class="line">                              generation_config=generation_config,</span><br><span class="line">                              safety_settings=safety_settings)</span><br><span class="line"></span><br><span class="line">prompt_parts = [</span><br><span class="line">  <span class="string">" You are a product marketer targeting a Gen Z audience. Create exciting and\nfresh advertising copy for products and their simple description. Keep copy\nunder a few sentences long."</span>,</span><br><span class="line">  <span class="string">"Product: 老式运动鞋"</span>,</span><br><span class="line">  <span class="string">"Product copy: 让我们系上安全带！这些鞋子的外观和颜色也别具一格，拥有别具一格的风格和功能。"</span>,</span><br><span class="line">  <span class="string">"Product: 超柔软连帽衫"</span>,</span><br><span class="line">  <span class="string">"Product copy: 穿上我们全新的男女通用连帽衫，舒适又时尚！这款连帽衫由 100% 棉制成，柔软舒适，全天佩戴。即使是在最冷的日子，里面的半刷墙也能让你保持温暖。"</span>,</span><br><span class="line">  <span class="string">"Product: 宽松卫衣"</span>,</span><br><span class="line">  <span class="string">"Product copy: "</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = model.generate_content(prompt_parts)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><h4 id="Chat-prompt"><a href="#Chat-prompt" class="headerlink" title="Chat prompt"></a>Chat prompt</h4><ol><li>可以提供prompt exmples, 定制你希望的 机器人聊天风格。</li><li>还可以把与定制的机器人的部分聊天内容，加入prompt exmples中，不断优化聊天风格。</li><li>推理时，会把chat prompts和 历史聊天内容 随机组合，作为few-shot加入对话前面。</li></ol><p><img src="/imgs/Pasted image 20231220092147.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">At the command line, only need to run once to install the package via pip:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ pip install google-generativeai</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> google.generativeai <span class="keyword">as</span> genai</span><br><span class="line"></span><br><span class="line">genai.configure(api_key=<span class="string">"YOUR_API_KEY"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Set up the model</span></span><br><span class="line">generation_config = &#123;</span><br><span class="line">  <span class="string">"temperature"</span>: <span class="number">0.9</span>,</span><br><span class="line">  <span class="string">"top_p"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="string">"top_k"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="string">"max_output_tokens"</span>: <span class="number">2048</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">safety_settings = [</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_HARASSMENT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_HATE_SPEECH"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_SEXUALLY_EXPLICIT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"category"</span>: <span class="string">"HARM_CATEGORY_DANGEROUS_CONTENT"</span>,</span><br><span class="line">    <span class="string">"threshold"</span>: <span class="string">"BLOCK_MEDIUM_AND_ABOVE"</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model = genai.GenerativeModel(model_name=<span class="string">"gemini-pro"</span>,</span><br><span class="line">                              generation_config=generation_config,</span><br><span class="line">                              safety_settings=safety_settings)</span><br><span class="line"></span><br><span class="line">convo = model.start_chat(history=[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"You are Tim, a friendly alien that lives on Europa, one of\nJupiter's moons."</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"Ok"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"Hi!"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"Hi! My name is Tim and I live on Europa, one of Jupiter's moons. Brr!\nIt's cold down here!"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"你好"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"这里很寂寞，可以和我一起玩吗？"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"你好啊"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"你好！很高兴认识你！你叫什么名字？"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"不告诉你"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"好吧，没关系。你愿意和我玩捉迷藏吗？我可以先躲起来，然后你来试着找我。我保证会藏在一个不会太难找的地方。"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"你喜欢我吗"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"role"</span>: <span class="string">"model"</span>,</span><br><span class="line">    <span class="string">"parts"</span>: <span class="string">"我当然喜欢你！你很有趣，也很聪明。我喜欢和你聊天。你让我对地球有了更多的了解，也让我对人类有了更多的认识。你是一个非常特别的人，我真的很喜欢你。"</span></span><br><span class="line">  &#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">convo.send_message(<span class="string">"YOUR_USER_INPUT"</span>)</span><br><span class="line">print(convo.last.text)</span><br></pre></td></tr></table></figure><h3 id="Usage-Examples"><a href="#Usage-Examples" class="headerlink" title="Usage Examples"></a>Usage Examples</h3><h4 id="海报分析"><a href="#海报分析" class="headerlink" title="海报分析"></a>海报分析</h4><p><img src="/imgs/d827c8d6gy1hhmzwdkyvhj20qo0xpaig.jpg" alt><br><img src="/imgs/Pasted image 20231219180002.png" alt><br>结论：效果极好。</p><h4 id="发票抽取"><a href="#发票抽取" class="headerlink" title="发票抽取"></a>发票抽取</h4><p><img src="/imgs/invoice.png" alt="img"><br><img src="/imgs/Pasted image 20231219175142.png" alt></p><p>结论：效果较差。</p><h3 id="Gemini-Pro-vs-GPT4"><a href="#Gemini-Pro-vs-GPT4" class="headerlink" title="Gemini Pro vs GPT4"></a>Gemini Pro vs GPT4</h3><h4 id="Gemini-Pro"><a href="#Gemini-Pro" class="headerlink" title="Gemini Pro"></a>Gemini Pro</h4><p><img src="/imgs/Pasted image 20231229144812.png" alt><br><img src="/imgs/Pasted image 20231229144820.png" alt><br><img src="/imgs/Pasted image 20231229144841.png" alt><br><img src="/imgs/Pasted image 20231229144847.png" alt><br><img src="/imgs/Pasted image 20231229144857.png" alt><br><img src="/imgs/Pasted image 20231229144903.png" alt><br><img src="/imgs/Pasted image 20231229144908.png" alt><br><img src="/imgs/Pasted image 20231229144912.png" alt></p><h4 id="GPT4"><a href="#GPT4" class="headerlink" title="GPT4"></a>GPT4</h4><p><img src="/imgs/Pasted image 20231229144958.png" alt><br><img src="/imgs/Pasted image 20231229145009.png" alt><br><img src="/imgs/Pasted image 20231229145020.png" alt><br><img src="/imgs/Pasted image 20231229144952.png" alt><br><img src="/imgs/Pasted image 20231229145028.png" alt><br>解析结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"发票代码"</span>: <span class="string">"033001700211"</span>,</span><br><span class="line">  <span class="string">"发票号码"</span>: <span class="string">"56894556"</span>,</span><br><span class="line">  <span class="string">"开票日期"</span>: <span class="string">"2018年01月08日"</span>,</span><br><span class="line">  <span class="string">"校验码"</span>: <span class="string">"59383 45927 24400 05323"</span>,</span><br><span class="line">  <span class="string">"购买方名称"</span>: <span class="string">"浙江金华市健八路建筑安装有限公司"</span>,</span><br><span class="line">  <span class="string">"纳税人识别号"</span>: <span class="string">"91330101768225450T"</span>,</span><br><span class="line">  <span class="string">"地址、电话"</span>: <span class="string">"金华市金东区健八路369号宝龙广场1号楼0571-87233885"</span>,</span><br><span class="line">  <span class="string">"开户行及账号"</span>: <span class="string">"上海浦东发展银行金华分行351304800003964"</span>,</span><br><span class="line">  <span class="string">"商品或应税劳务、服务名称"</span>: <span class="string">"咨询费"</span>,</span><br><span class="line">  <span class="string">"规格型号"</span>: <span class="string">"无"</span>,</span><br><span class="line">  <span class="string">"单位"</span>: <span class="string">"项"</span>,</span><br><span class="line">  <span class="string">"数量"</span>: <span class="string">"1"</span>,</span><br><span class="line">  <span class="string">"单价"</span>: <span class="string">"66.97"</span>,</span><br><span class="line">  <span class="string">"金额"</span>: <span class="string">"66.97"</span>,</span><br><span class="line">  <span class="string">"税率"</span>: <span class="string">"***"</span>,</span><br><span class="line">  <span class="string">"税额"</span>: <span class="string">"0.03"</span>,</span><br><span class="line">  <span class="string">"价税合计(大写)"</span>: <span class="string">"陆拾柒元整"</span>,</span><br><span class="line">  <span class="string">"价税合计(小写)"</span>: <span class="string">"67.00"</span>,</span><br><span class="line">  <span class="string">"销售方名称"</span>: <span class="string">"浙江金华市健八路建筑安装有限公司"</span>,</span><br><span class="line">  <span class="string">"销售方纳税人识别号"</span>: <span class="string">"91330100552660066F"</span>,</span><br><span class="line">  <span class="string">"销售方地址、电话"</span>: <span class="string">"金华市金东区健八路369号宝龙广场1号楼0571-87233885"</span>,</span><br><span class="line">  <span class="string">"销售方开户行及账号"</span>: <span class="string">"上海浦东发展银行金华分行351304800003964"</span>,</span><br><span class="line">  <span class="string">"备注"</span>: <span class="string">"外贸咨询费"</span>,</span><br><span class="line">  <span class="string">"收款人"</span>: <span class="string">"周丽君"</span>,</span><br><span class="line">  <span class="string">"复核"</span>: <span class="string">"周丽君"</span>,</span><br><span class="line">  <span class="string">"开票人"</span>: <span class="string">"糜忠良"</span>,</span><br><span class="line">  <span class="string">"销售方"</span>: <span class="string">"(章)"</span>,</span><br><span class="line">  <span class="string">"发票状态"</span>: <span class="string">"发票查验"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/imgs/Pasted image 20231229145252.png" alt><br>Refs:</p><blockquote><p><a href="https://ai.google.dev/tutorials/ai-studio_quickstart?hl=zh-cn#structured_example" target="_blank" rel="noopener">Google AI Studio 快速入门  |  Google AI for Developers</a><br><a href="https://deepmind.google/technologies/gemini/#introduction" target="_blank" rel="noopener">Gemini - Google DeepMind</a><br><a href="https://www.youtube.com/watch?v=UIZAiXYceBI&amp;ab_channel=Google" target="_blank" rel="noopener">Hands-on with Gemini: Interacting with multimodal AI - YouTube</a></p></blockquote><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h3 id="GPT4-Turbo"><a href="#GPT4-Turbo" class="headerlink" title="GPT4 Turbo"></a>GPT4 Turbo</h3><h4 id="New-Features"><a href="#New-Features" class="headerlink" title="New Features:"></a>New Features:</h4><ol><li>Context length: 12.8万个上下文tokens。(GPT4 support 8K sometimes 32K tokens)</li><li>More control:<ol><li>新加了 Json Model.<img src="/imgs/Pasted image 20231114161927.png" alt> 其将返回json格式（这使调用api变得更容易）。</li><li>调用函数更好：<img src="/imgs/Pasted image 20231114162447.png" alt></li><li>可复制输出，传递种子参数，它使模型返回一致的输出，使你对模型行为有更高程度的控制。</li></ol></li><li>Better knowledge:<ol><li>可以讲外部文档或数据库中的知识 带入你正在构建的任何内容。</li><li>拥有截止23.4月的世界知识。</li></ol></li><li>New modalities:<ol><li>DALL·E 3 , GPT-4, TTS 融合。</li><li>可通过api接受图像的输入，生成字幕，分类，分析。（应用：Be My Eyes 使用此 来帮助盲人）</li><li>文本到语音模型，能够产生逼真的自然声音，有<strong>六种</strong>预设声音可选：<ol><li>语音可使应用程序更自然地交互，更容易访问；</li><li>还可 应用到 语言学习和语音助手。</li></ol></li><li>更新了开源语音识别模型：Whisper V3</li></ol></li><li>Customization: 可定制化为公司开发GPT。</li><li>Higher rate limits: 2x tokens per minute.</li></ol><h3 id="GPTs"><a href="#GPTs" class="headerlink" title="GPTs"></a>GPTs</h3><ol><li>为特定目的量身定制的GPT版本(如创业导师)。</li><li>可发布给别人使用。</li><li>结合了 Instructions, Expanded knowledge and Actions.</li></ol><h2 id="名人言论"><a href="#名人言论" class="headerlink" title="名人言论"></a>名人言论</h2><h3 id="OpenAI首席科学家Ilya-Sutskever"><a href="#OpenAI首席科学家Ilya-Sutskever" class="headerlink" title="OpenAI首席科学家Ilya Sutskever"></a>OpenAI首席科学家Ilya Sutskever</h3><ol><li>Nature将其评为「2023年10大科学人物」。</li><li>曾多次强调：只要能够非常好得预测下一个token，就能帮助人类达到AGI。</li><li>就像统计学一样，为了理解这些统计数据并对其进行压缩，你需要了解创建这组统计数据的世界是什么？</li><li>OpenAI为什么放弃了机器人的方向？这是一个循序渐进的改进过程，需要建造更多机器人，收集更多数据。为了实现机器人技术的发展，必须全身心投入，并愿意解决所有相关的物理和后勤问题。这与纯粹的软件开发完全不同。只要有足够的努力和热情，机器人技术是有可能取得重大进步的，而且已经有一些公司在这方面做出了努力。</li></ol><blockquote><p><a href="https://mp.weixin.qq.com/s/i7rWeQyVoulNjvcwmkcgyg" target="_blank" rel="noopener">前OpenAI首席科学家Ilya: 只要能够预测下一个token，人类就能达到AGI</a></p></blockquote><h3 id="谷歌DeepMind研究副总裁Pushmeet-Kohli"><a href="#谷歌DeepMind研究副总裁Pushmeet-Kohli" class="headerlink" title="谷歌DeepMind研究副总裁Pushmeet Kohli"></a>谷歌DeepMind研究副总裁Pushmeet Kohli</h3><ol><li>用大模型解决困扰数学家60多年的问题，谷歌DeepMind最新成果再登Nature。</li><li>其表示：训练数据中不会有这个方案，它之前甚至根本不为人类所知。</li></ol><h2 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h2><ol><li>掌握 可以 直接有生产力、产生价值的 技术，而非 纠结于技术细节本身，用进废退，价值最大化。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对当前（2023.12.30）大模型进行调研。&lt;/p&gt;
    
    </summary>
    
      <category term="LargeModel" scheme="https://huzhenyou.github.io/categories/LargeModel/"/>
    
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
      <category term="LargeModel" scheme="https://huzhenyou.github.io/tags/LargeModel/"/>
    
  </entry>
  
  <entry>
    <title>mmaction2之demo_spatiotemporal_det.py分析</title>
    <link href="https://huzhenyou.github.io//blog/2023/09/mmaction2.html"/>
    <id>https://huzhenyou.github.io//blog/2023/09/mmaction2.html</id>
    <published>2023-09-17T07:34:05.000Z</published>
    <updated>2024-01-10T02:23:42.151Z</updated>
    
    <content type="html"><![CDATA[<p>mmaction人体行为检测脚本demo/demo_spatiotemporal_det.py的分析。</p><a id="more"></a><hr><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>对人体行为检测demo脚本demo/demo_spatiotemporal_det.py进行分析。</p><h1 id="extract-all-frames"><a href="#extract-all-frames" class="headerlink" title="extract all frames"></a>extract all frames</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frame_paths, original_frames = frame_extract(args.video, </span><br><span class="line">out_dir=tmp_dir.name) <span class="comment"># extract all frames</span></span><br></pre></td></tr></table></figure><h1 id="sample-interval"><a href="#sample-interval" class="headerlink" title="sample interval"></a>sample interval</h1><ol><li>window_size = clip_len * frame_interval, here clip_len is the number of frames in the window,  frame_interval is the interval of frames in window.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">window_size = clip_len * frame_interval</span><br><span class="line"><span class="keyword">assert</span> clip_len % <span class="number">2</span> == <span class="number">0</span>, <span class="string">'We would like to have an even clip_len'</span></span><br><span class="line"><span class="comment"># Note that it's 1 based here</span></span><br><span class="line">timestamps = np.arange(window_size // <span class="number">2</span>, num_frame + <span class="number">1</span> - window_size // <span class="number">2</span>,</span><br><span class="line">   args.predict_stepsize)</span><br></pre></td></tr></table></figure><h1 id="human-detections"><a href="#human-detections" class="headerlink" title="human detections"></a>human detections</h1><ol><li>inference:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">human_detections, _ = detection_inference(args.det_config,</span><br><span class="line">                                              args.det_checkpoint,</span><br><span class="line">                                              center_frames,</span><br><span class="line">                                              args.det_score_thr,</span><br><span class="line">                                              args.det_cat_id, args.device)</span><br></pre></td></tr></table></figure><ol><li>restore to the original size:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(human_detections)):</span><br><span class="line">det = human_detections[i]</span><br><span class="line">det[:, <span class="number">0</span>:<span class="number">4</span>:<span class="number">2</span>] *= w_ratio</span><br><span class="line">det[:, <span class="number">1</span>:<span class="number">4</span>:<span class="number">2</span>] *= h_ratio</span><br><span class="line">human_detections[i] = torch.from_numpy(det[:, :<span class="number">4</span>]).to(args.device)</span><br></pre></td></tr></table></figure><h1 id="SpatioTemporal-Action-Detection"><a href="#SpatioTemporal-Action-Detection" class="headerlink" title="SpatioTemporal Action Detection"></a>SpatioTemporal Action Detection</h1><ol><li>get  all frames in a window according to a target frame(<code>timestamp</code>):</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_frame = timestamp - (clip_len // <span class="number">2</span> - <span class="number">1</span>) * frame_interval</span><br><span class="line">frame_inds = start_frame + np.arange(<span class="number">0</span>, window_size, frame_interval)</span><br><span class="line">frame_inds = list(frame_inds - <span class="number">1</span>)</span><br><span class="line">imgs = [frames[ind].astype(np.float32) <span class="keyword">for</span> ind <span class="keyword">in</span> frame_inds]</span><br></pre></td></tr></table></figure><ol><li>get the result of SpatioTemporal Action Detection:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.stack(imgs).transpose((<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))[np.newaxis]</span><br><span class="line">input_tensor = torch.from_numpy(input_array).to(args.device)</span><br><span class="line"></span><br><span class="line">datasample = ActionDataSample()</span><br><span class="line">datasample.proposals = InstanceData(bboxes=proposal)</span><br><span class="line">datasample.set_metainfo(dict(img_shape=(new_h, new_w)))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">result = model(input_tensor, [datasample], mode=<span class="string">'predict'</span>)</span><br><span class="line">scores = result[<span class="number">0</span>].pred_instances.scores</span><br><span class="line">prediction = []</span><br><span class="line"><span class="comment"># N proposals</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(proposal.shape[<span class="number">0</span>]):</span><br><span class="line">prediction.append([])</span><br><span class="line"><span class="comment"># Perform action score thr</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(scores.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_map:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(proposal.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">if</span> scores[j, i] &gt; args.action_score_thr:</span><br><span class="line">prediction[j].append((label_map[i], scores[j,i].item()))</span><br><span class="line">predictions.append(prediction)</span><br></pre></td></tr></table></figure><h1 id="Use-result"><a href="#Use-result" class="headerlink" title="Use result"></a>Use result</h1><ol><li>Show result in dense frames.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_timestamps</span><span class="params">(timestamps, n)</span>:</span></span><br><span class="line"><span class="string">"""Make it nx frames."""</span></span><br><span class="line">old_frame_interval = (timestamps[<span class="number">1</span>] - timestamps[<span class="number">0</span>])</span><br><span class="line">start = timestamps[<span class="number">0</span>] - old_frame_interval / n * (n - <span class="number">1</span>) / <span class="number">2</span> </span><br><span class="line">new_frame_inds = np.arange(</span><br><span class="line">len(timestamps) * n) * old_frame_interval / n + start</span><br><span class="line"><span class="keyword">return</span> new_frame_inds.astype(np.int64)</span><br></pre></td></tr></table></figure><p>here the meaning of <code>start = timestamps[0] - old_frame_interval / n * (n - 1) / 2</code>:</p><p><img src="/imgs/Pasted image 20230912092443.png" alt></p><h1 id="修复代码中的问题"><a href="#修复代码中的问题" class="headerlink" title="修复代码中的问题"></a>修复代码中的问题</h1><h2 id="加载大视频"><a href="#加载大视频" class="headerlink" title="加载大视频"></a>加载大视频</h2><p>问题描述: 当处理高清晰图像时，会在运行时崩溃掉。<br>原因如下：</p><ol><li>如我要处理的视频共510M, 但在 [[#extract all frames]] 这一步，把所有视频帧保存下来后，竟占用40多G，原因是视频进行了编码（类似于进行了压缩）。所以若遇到崩溃，首先看下，自己的磁盘空间是否足够。</li><li>更可气的是，每帧大小为(1440, 2560, 3)的图片，磁盘空间占用34G, 而内存却占用了309G, 原因是:<ol><li>内存 VS 硬盘( sys.getsizeof() VS os.path.getsize() ):  当加载一个文件到内存中时，用sys.getsizeof()获取变量大小，其包括文件数据及可能的其他python对象所占用的内存空间, 所以通常会远远大于文件在磁盘上的实际大小。</li></ol></li></ol><p>解决方案：对代码进行改造，分段进行识别。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mmaction人体行为检测脚本demo/demo_spatiotemporal_det.py的分析。&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>TriDet</title>
    <link href="https://huzhenyou.github.io//blog/2023/09/TriDet.html"/>
    <id>https://huzhenyou.github.io//blog/2023/09/TriDet.html</id>
    <published>2023-09-10T07:34:05.000Z</published>
    <updated>2023-09-10T09:15:22.205Z</updated>
    
    <content type="html"><![CDATA[<p>The detail of TriDet.</p><a id="more"></a><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ol><li>TriDet: Temporal Action Detection <strong>with</strong> Relative Boundary Modeling.</li><li><p>Temporal Action Detection(TAD)</p><ol><li>Detect all action boundaries and categories from an untrimmed video.<br><img src="/imgs/Pasted image 20230815192206.png" alt></li><li>The pipeline of TAD:<br><img src="/imgs/Pasted image 20230815192350.png" alt><ol><li>backbone:</li><li>use the pre-trained model in [[PoseEstimation_TAD#行为识别(Action Detection/Regnition)|Action Recognition Task]].</li><li>to get the feature map of each frame.</li></ol></li></ol></li><li><p>The core Focus of the author:</p><ol><li>get more accuracy boundary.</li><li>explore Transformer for TAD.</li></ol></li></ol><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h2><ol><li>two classes by the method of boundary dividing for TAD works using Transformer:<ol><li>[[#Segment-level prediction]]</li><li>[[#Instance-level prediction]]</li></ol></li></ol><h2 id="Segment-level-prediction"><a href="#Segment-level-prediction" class="headerlink" title="Segment-level prediction"></a>Segment-level prediction</h2><ol><li>based on extracted feature maps, get a clip, and simple, global expression(e.g., pooling), finally judge whether the clip is the target.</li><li>e.g. both of the below are two-stage network(like Faster-Rcnn, one stage generate lots of proposals, the second stage regresses and classifies proposals):<ol><li>[[BMNDetail|BMN]]</li><li>PGCN：<ol><li>use GCN to refine every proposal.</li></ol></li></ol></li><li>The method can’t be trained end-to-end.</li><li>End-to-end:<ol><li>TadTR</li><li>ReAct</li></ol></li></ol><h2 id="Instance-level-prediction"><a href="#Instance-level-prediction" class="headerlink" title="Instance-level prediction"></a>Instance-level prediction</h2><h3 id="Anchor-free-Detection"><a href="#Anchor-free-Detection" class="headerlink" title="Anchor-free Detection"></a>Anchor-free Detection</h3><h4 id="AFSD"><a href="#AFSD" class="headerlink" title="AFSD"></a>AFSD</h4><p><img src="/imgs/Pasted image 20230824200202.png" alt></p><ol><li>predict the distance to the start or end boundary.</li><li>then make the position pointed by most other position as the boundary.</li></ol><h4 id="ActionFormer"><a href="#ActionFormer" class="headerlink" title="ActionFormer"></a>ActionFormer</h4><p><img src="/imgs/Pasted image 20230824195951.png" alt></p><ol><li>Using slide-window to apply self-attention.</li><li>In 2022, the work in TAD improved obviously.</li><li>So TriDet carry out based on the work.</li></ol><h3 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h3><p><img src="/imgs/Pasted image 20230824201525.png" alt></p><h4 id="MLAD"><a href="#MLAD" class="headerlink" title="MLAD"></a>MLAD</h4><ol><li>Apply self-attention in the time level dimension and the classes level dimension.</li><li>Add the two attentions of  the two level dimensions to build the final feature map.</li></ol><h4 id="MS-TCT"><a href="#MS-TCT" class="headerlink" title="MS-TCT"></a>MS-TCT</h4><ol><li>Add a CNN module after a traditional self-attention module.</li><li>Add residual connections.</li></ol><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol><li>For the above  two works,  they both pay attention to modifying  the self-attention.</li><li>So, it indicates that the original self-attention cannot be applied to TAD.</li></ol><h2 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><h3 id="Segment-level-prediction-1"><a href="#Segment-level-prediction-1" class="headerlink" title="Segment-level prediction"></a>Segment-level prediction</h3><ol><li>Contain global representation of segments.</li><li>Larger receptive field.</li><li>More information.</li><li>Detailed Information at each instant is discarded.</li><li>Highly depend on the accuracy of segments.<br><img src="/imgs/Pasted image 20230829191215.png" alt></li></ol><h3 id="Instant-level-prediction"><a href="#Instant-level-prediction" class="headerlink" title="Instant-level prediction"></a>Instant-level prediction</h3><ol><li>Contain detailed representation of instants.</li><li>Smaller receptive field.</li><li>The requirement for feature distinguishability is high. (use the strong backbone to extract features.)</li><li>The degree of response varies greatly with different videos.</li></ol><h1 id="Motivation-of-Trident-head"><a href="#Motivation-of-Trident-head" class="headerlink" title="Motivation of Trident-head"></a>Motivation of Trident-head</h1><ol><li>Consider both instant-level and segment-level feature.</li><li>Set it as the segment-level feature that the predicted frame with the  fixed number of adjacent frames.</li><li>Set the segment-level feature as instant-level feature?</li></ol><h1 id="Trident-head"><a href="#Trident-head" class="headerlink" title="Trident-head"></a>Trident-head</h1><p><img src="/imgs/Pasted image 20230904090904.png" alt></p><ol><li>Three branch:<ol><li><span style="color:#FF8C00;">Start Boundary </span>  and <span style="color:#FF8C00;">End Boundary </span> would extract the segment-level feature.</li><li><span style="color:#FF8C00;">Center Offset </span> would extract instance-level feature.</li></ol></li><li>E.G., Predicted <span style="color:Purple;">Start Boundary</span>  is decided by <span style="color:#FF8C00;">Start Boundary </span> and  <span style="color:#FF8C00;">Center Offset </span>:<br><img src="/imgs/Pasted image 20230904182444.png" alt></li><li>Expectation is decided by B:<ol><li>if B is too small, we can’t find the more far boundary.</li><li>if B is too big, the difficulty of  learning and convergence of training is more great, so that the predicted result is not accuracy.</li></ol></li><li>Combined with FPN:<br><img src="/imgs/Pasted image 20230904184206.png" alt><ol><li>In the different level layers, the fixed number of B is set to product different Bs, so it can have small and big Bs simultaneously.</li><li>While finally outputting, the predicted results in different layers times corresponding scale ratio to get real position.</li></ol></li></ol><h1 id="The-second-question-Attention-in-Temporal-Dimension"><a href="#The-second-question-Attention-in-Temporal-Dimension" class="headerlink" title="The second question: Attention in Temporal Dimension."></a>The second question: Attention in Temporal Dimension.</h1><ol><li>Many methods require complex attention mechanisms to make the network work.</li><li>The success of the previous transformer-based layers(in TAD) primarily relies on their macro-architecture, rather than the self-attention mechanism.<br><img src="/imgs/Pasted image 20230904193650.png" alt></li><li>Above, when 1D-Conv take the place of Self-attention, the Avg map only drops by 1.9, but  when CNN baseline takes the place of Transformer baseline, the Avg map drops very much,  which indicates that Transformer is effective depending to its structure not Self-attention.<br><img src="/imgs/4. ![[The Rank Loss Problem of Self Attention" alt><br>In TAD, making the features same is disastrous, because we need to distinguish a position is the action or not.</li><li><p>Pure LayerNorm will normalize the features <script type="math/tex">x\in R^n</script> to a modulus <script type="math/tex">\sqrt{n}</script> :</p><script type="math/tex; mode=display">x' = LayerNorm(x)</script><script type="math/tex; mode=display">x'_i = \frac{x_i-mean(x)}{\frac{1}{n} {\textstyle \sum_{n}^{}(x_i-mean(x))^2}}</script><script type="math/tex; mode=display">\left \| x' \right \| ^2_2 = {\textstyle \sum_{n}^{}x'^2_i}=n</script></li><li><p>The Evidence on HACS:<br>we consider the cosine similarity:<br><img src="/imgs/Pasted image 20230904203951.png" alt></p><ol><li>here SA is Self-Attention, SGP is proposed by the author, BackB is the backbone network to extract the feature.</li><li>here the value is the average of the cosine similarity of every feature and average feature in the same layer ?</li></ol></li><li>Consider the Self Attention:<script type="math/tex; mode=display">V'=WV</script><script type="math/tex; mode=display">W=Softmax(\frac{QK^T}{\sqrt{d}})</script>W is non-negative and the sum of each row is 1, thus the <script type="math/tex">V'</script> are [[ConceptAI#convex combination|convex combination]] for the input <script type="math/tex">V</script>.<br>But the value in Convolution kernel can be negative, and the sum can be not 1.</li></ol><h1 id="The-author’s-Solution-The-SGP-layer"><a href="#The-author’s-Solution-The-SGP-layer" class="headerlink" title="The author’s Solution: The SGP layer"></a>The author’s Solution: The SGP layer</h1><ol><li>increase the discrimination of feature.</li><li><p>capture temporal information with different scales of receptive fields.</p><script type="math/tex; mode=display">f_{SGP}=\Phi (x)FC(x)+\psi(x)(Conv_w(x)+Conv_{kw}(x))+x$$,$$\Phi(x)=ReLU(FC(AvgPool(x)))$$,$$\psi(x)=Conv_w(x)</script><p>Window-level: make the network extract features in different scales adaptively.</p><p>In detail:</p><ol><li>the author uses the depth-wise convolution to reduce the computation of the network.</li><li>add a additional residual connection.</li></ol></li></ol><p><img src="/imgs/Pasted image 20230904211744.png" alt></p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>The performance is so good: the accuracy is higher, the speed is faster.</p><p><img src="/imgs/Pasted image 20230905084929.png" alt></p><p><img src="/imgs/Pasted image 20230905085657.png" alt></p><h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ol><li><a href="https://www.bilibili.com/video/BV12M4y117GZ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2c23be48ba22c91130ce4868020ab598" target="_blank" rel="noopener">https://www.bilibili.com/video/BV12M4y117GZ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2c23be48ba22c91130ce4868020ab598</a>  (‘4.10)</li><li>Paper: <a href="https://arxiv.org/abs/2303.07347" target="_blank" rel="noopener">https://arxiv.org/abs/2303.07347</a></li><li>Code: <a href="https://github.com/dingfengshi/TriDet" target="_blank" rel="noopener">https://github.com/dingfengshi/TriDet</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The detail of TriDet.&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
      <category term="CVPR2023" scheme="https://huzhenyou.github.io/tags/CVPR2023/"/>
    
      <category term="TemporalActionDetection" scheme="https://huzhenyou.github.io/tags/TemporalActionDetection/"/>
    
  </entry>
  
  <entry>
    <title>BMN详解</title>
    <link href="https://huzhenyou.github.io//blog/2023/08/BMN%E8%AF%A6%E8%A7%A3.html"/>
    <id>https://huzhenyou.github.io//blog/2023/08/BMN详解.html</id>
    <published>2023-08-13T07:34:05.000Z</published>
    <updated>2023-09-10T09:02:13.160Z</updated>
    
    <content type="html"><![CDATA[<p>BMN(Boundary-Matching Network) 详解。</p><a id="more"></a><hr><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>百度，ActivityNet Challenge 2019 冠军模型：BMN: Boundary-Matching Network for Temporal Action Proposal Generation。</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><ol><li>Unlike temporal action detection task, in the work categories of action instances are not taken into account in proposal generation task.</li><li>The temporal annotation: <script type="math/tex">\Psi_g=\left \{ \varphi_n=(t_{s,n}, t_{e,n})  \right \} ^{N_g}_{n=1}</script>, here <script type="math/tex">N_g</script> is  the amount of ground-truth action instances.</li><li>During inference, proposal generation method should generate proposals <script type="math/tex">\Psi_p</script> which cover  <script type="math/tex">\Psi_g</script> precisely and exhaustively.</li></ol><h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p><img src="/imgs/Pasted image 20230807194737.png" alt><br><img src="/imgs/Pasted image 20230807194836.png" alt></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li>Feature Extraction: 使用双流网络（光流+RGB），获得feature map.</li><li>Base Module： 1x1卷积（时序卷积）。</li><li>Temporal Evaluation Module(TEM)： 1x1卷积（时序卷积），获得开始、结束点的概率序列。</li><li>Proposal Evaluation Module(PEM)：<ol><li>[[PapersRead#BMN#BM layer|BM layer]]</li><li>通过conv3d, conv2d 得到置信度图。</li></ol></li><li>生成结果：<ol><li>把两条边界概率序列中大于<script type="math/tex">极大值\times  \frac{1}{2}</script> 或是峰值（极大值）的都看作开始或结束边界.</li><li>$n^2$复杂度两两组合，获得一系列proposals:</li><li>根据置信度图获得各个proposals的置信度。<ol><li>the proposal denoted: <script type="math/tex">φ = (t_s, t_e, p^s_{ts} , p^e_{te} , p_{cc}, p_{cr})</script><br>  where <script type="math/tex">p^s_{ts},  p^e_{te}</script> are starting and ending probabilities.  <script type="math/tex">p_{cc}, p_{cr}</script> are classification confidence and regression confidence score.</li><li>get final score: <script type="math/tex">p_f = p^s_{ts} · p^e_{te} · \sqrt{p_{cc}· p_{cr})}</script></li></ol></li><li>利用Soft-NMS去冗余。</li></ol></li></ol><h2 id="置信度图："><a href="#置信度图：" class="headerlink" title="置信度图："></a>置信度图：</h2><p><img src="/imgs/Pasted image 20230807202541.png" alt></p><ol><li><script type="math/tex">M_C\in R^{D×T}</script>.</li><li>由开始点和长度决定结束点，从而确定一个proposal. 所以上图对应所有任意视频段的置信度。</li><li>duration dim: proposal长度.</li><li>starting dim: 开始点位置。</li><li>同一行对应的proposals对应相同的长度。同一列队对应的proposals拥有相同的开始点。同一负对角线对应的proposals拥有相同的结束边界。右下角部分proposals超出视频范围，无意义。</li></ol><h2 id="BM-layer"><a href="#BM-layer" class="headerlink" title="BM layer"></a>BM layer</h2><ol><li>The goal: uniformly sample N points in <script type="math/tex">S_{F} ∈ R^{C×T}</script>  between starting boundary <script type="math/tex">t_{s}</script> and ending boundary <script type="math/tex">t_{e}</script> of each proposal <script type="math/tex">φ_{i,j}</script>, and get proposal feature <script type="math/tex">m^f_{i,j} ∈ R^{C×N}</script> with rich context (actually sampling in [<script type="math/tex">t_S-0.25d, t_e+0.25d</script>]).<br> <img src="/imgs/Pasted image 20230810081026.png" alt></li><li>two problems: <ol><li>how to sample feature in non-integer point:<br> ![[Pasted image 20230808205131.png|375]]</li><li>how to sample feature for all proposals simultaneously:<ol><li>expanding <script type="math/tex">w_{i,j} ∈ R^{N ×T}</script>  to  <script type="math/tex">W ∈ R^{N ×T ×D×T}</script> for all proposals in BM confidence map.</li><li>get <script type="math/tex">M_F ∈ R^{C×N×D×T}</script> by using dot product:   <script type="math/tex">S_{F} ∈ R^{C×T}</script> and  <script type="math/tex">W^T ∈ R^{T×N×D×T}</script>. （<em><script type="math/tex">W</script> can be pre-generated because it’s the same for different videos, the inference speed of BM layer is very fast.</em> Is T is the same for the different videos? Ans: [[#BMN#Base module|Base module]] and [[#BMN#Training of BMN#Training Data Construction|Training Data Construction]]. TODO: review code）</li></ol></li></ol></li></ol><h2 id="Base-module"><a href="#Base-module" class="headerlink" title="Base module"></a>Base module</h2><ol><li><em>adopt a long observation window with length <script type="math/tex">l_ω</script> to truncate the untrimmed feature sequence with length <script type="math/tex">l_f</script> .</em></li><li>So here <script type="math/tex">l_w</script> is <script type="math/tex">T</script> in <script type="math/tex">S_{F} ∈ R^{C×T}</script>.</li></ol><h2 id="Proposal-Evaluation-Module-PEM"><a href="#Proposal-Evaluation-Module-PEM" class="headerlink" title="Proposal Evaluation Module(PEM)"></a>Proposal Evaluation Module(PEM)</h2><ol><li>Final generate: <script type="math/tex">M_C\in R^{D×T}</script>, but there are two predicted <script type="math/tex">M_C</script>: <script type="math/tex">M_{CC}</script>, <script type="math/tex">M_{CR}</script>, being trained using binary classification and regression loss function separately. TODO: review code.</li></ol><h2 id="Training-of-BMN"><a href="#Training-of-BMN" class="headerlink" title="Training of BMN"></a>Training of BMN</h2><h3 id="TEM-vs-PEM"><a href="#TEM-vs-PEM" class="headerlink" title="TEM vs PEM:"></a>TEM vs PEM:</h3><ol><li>TEM: learns local boundary.</li><li>PEM: pattern global proposal context.</li></ol><h3 id="Training-Data-Construction"><a href="#Training-Data-Construction" class="headerlink" title="Training Data Construction:"></a>Training Data Construction:</h3><ol><li>firstly, extract all feature sequence F with length.</li><li>get many observation windows with length <script type="math/tex">l_w</script> with 50% overlap.</li><li>here every window contains at least one ground-truth action instance.</li></ol><h3 id="Label-Assignment"><a href="#Label-Assignment" class="headerlink" title="Label Assignment"></a>Label Assignment</h3><h4 id="TEM"><a href="#TEM" class="headerlink" title="TEM"></a>TEM</h4><ol><li>denote its starting and ending regions as <script type="math/tex">r_S = [t_s − d_g /10, t_s +d_g/10]</script> and $r_E =[t_e−d_g/10,t_e+d_g/10]$separately.</li><li>denote its local region as <script type="math/tex">r_{t_n} = [t_n −d_f /2, t_n +d_f /2]</script>, where <script type="math/tex">d_f = t_n −t_{n−1}</script> is the temporal interval between two locations. </li><li>Then calculate overlap ratio IoR of <script type="math/tex">r_{t_n}</script> with <script type="math/tex">r_S</script> and <script type="math/tex">r_E</script> separately, and denote maximum IoR as <script type="math/tex">g^s_{t_n}</script> and  <script type="math/tex">g^e_{t_n}</script> separately.</li><li><em>here IoR is defined as the overlap ratio with ground-truth proportional to the duration of this region.</em> TODO: code review.</li><li>Thus generate <script type="math/tex">G_{S,ω}=\{g^s_{t_n}\}^{l_w}_{n=1}</script> and <script type="math/tex">G_{E,ω}=\{g^e_{t_n}\}^{l_w}_{n=1}</script>  as label of TEM.</li></ol><h4 id="PEM"><a href="#PEM" class="headerlink" title="PEM"></a>PEM</h4><ol><li>Purpose: BM label map <script type="math/tex">G_C ∈ R^{D×T}</script>.</li><li>For a proposal <script type="math/tex">φ_{i,j}=(t_s=t_j, t_e=t_j+t_i)</script> , calculate its IoU with all <script type="math/tex">φ_g</script> in <script type="math/tex">Ψ_ω</script>, and denote the maximum IoU as <script type="math/tex">g^c_{i,j}</script> . Thus we can generate <script type="math/tex">G_C=\{g^c_{i,j}\}^{D,l_ω}_{i,j=1}</script>  as label of PEM.</li></ol><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Loss-of-TEM"><a href="#Loss-of-TEM" class="headerlink" title="Loss of TEM"></a>Loss of TEM</h4><ol><li>adopt weighted binary logistic regression loss function <script type="math/tex">L_{bl}</script>,  to get the sum of starting and ending losses:<script type="math/tex; mode=display">L_{TEM} =L_{bl}(P_S,G_S)+L_{bl}(P_E,G_E)</script></li><li><script type="math/tex; mode=display">L_{bl}(P,G): \frac{1}{l_w}\sum_{i=1}^{l_w}(a^+·b_i·log(p_i)+a^-·(1-b_i)·log(1-p_i))</script> where <script type="math/tex">b_i = sign(g_i − θ)</script> is a two-value function used to convert <script type="math/tex">g_i</script> from [0, 1] to {0, 1}0, 1} based on overlap threshold$θ = 0.5$. Denoting <script type="math/tex">l^+=\sum b_i</script>  and <script type="math/tex">l^− = l_ω −l^+</script>, the weighted terms are <script type="math/tex">α^+ = \frac{l_w}{l^+}</script> and <script type="math/tex">α^- = \frac{l_w}{l^-}</script>.</li></ol><h4 id="Loss-of-PEM"><a href="#Loss-of-PEM" class="headerlink" title="Loss of PEM"></a>Loss of PEM</h4><ol><li>Define:<script type="math/tex; mode=display">L_{PEM} =L_C(M_{CC},G_C)+λ·L_R(M_{CR},G_C)</script><ol><li>here  <script type="math/tex">L_{bl}</script>  for <script type="math/tex">L_C</script>  ,  L2 loss for <script type="math/tex">L_R</script> .  <script type="math/tex">λ = 10</script> . </li><li>to balance the ratio between positive and negative samples in <script type="math/tex">L_R</script> , take all points with <script type="math/tex">g^C_{i,j}>0.6</script>  as positive, and randomly sample  <script type="math/tex">g^C_{i,j}<0.2</script>  as negative, ensure 1:1 for positive: negative.</li></ol></li></ol><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><script type="math/tex; mode=display">L=L_{LEM} +λ_1 ·L_{GEM} +λ_2 ·L_2(Θ)</script><p>where <script type="math/tex">L_2(Θ)</script> is L2 regularization term,  <script type="math/tex">λ_1</script>, <script type="math/tex">λ_2</script> are set to 1, 0.000 to ensure different modules are trained evenly.</p><p>Refs: </p><ol><li><a href="https://zhuanlan.zhihu.com/p/337432552" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/337432552</a></li><li><a href="https://arxiv.org/pdf/1907.09702.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.09702.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BMN(Boundary-Matching Network) 详解。&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/tags/ActionDetection/"/>
    
      <category term="ProposalGeneration" scheme="https://huzhenyou.github.io/tags/ProposalGeneration/"/>
    
      <category term="ActivityNetChallenge2019" scheme="https://huzhenyou.github.io/tags/ActivityNetChallenge2019/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
  </entry>
  
</feed>
