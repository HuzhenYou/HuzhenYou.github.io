<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>九万里</title>
  
  <subtitle>虚怀若谷，大智若愚</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://huzhenyou.github.io/"/>
  <updated>2023-08-13T07:38:59.074Z</updated>
  <id>https://huzhenyou.github.io/</id>
  
  <author>
    <name>AngryBirds</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BMN详解</title>
    <link href="https://huzhenyou.github.io//blog/2023/08/BMN%E8%AF%A6%E8%A7%A3.html"/>
    <id>https://huzhenyou.github.io//blog/2023/08/BMN详解.html</id>
    <published>2023-08-13T07:34:05.000Z</published>
    <updated>2023-08-13T07:38:59.074Z</updated>
    
    <content type="html"><![CDATA[<p>BMN(Boundary-Matching Network) 详解。</p><a id="more"></a><hr><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>百度，ActivityNet Challenge 2019 冠军模型：BMN: Boundary-Matching Network for Temporal Action Proposal Generation。</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><ol><li>Unlike temporal action detection task, in the work categories of action instances are not taken into account in proposal generation task.</li><li>The temporal annotation:  <script type="math/tex">\Psi_g=\left \{ \varphi_n=(t_{s,n}, t_{e,n})  \right \} ^{N_g}_{n=1}</script> , here <script type="math/tex">N_g</script> is  the amount of ground-truth action instances.</li><li>During inference, proposal generation method should generate proposals <script type="math/tex">\Psi_p</script> which cover  <script type="math/tex">\Psi_g</script> precisely and exhaustively.</li></ol><h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p><img src="/imgs/Pasted image 20230807194737.png" alt><br><img src="/imgs/Pasted image 20230807194836.png" alt></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li>Feature Extraction: 使用双流网络（光流+RGB），获得feature map.</li><li>Base Module： 1x1卷积（时序卷积）。</li><li>Temporal Evaluation Module(TEM)： 1x1卷积（时序卷积），获得开始、结束点的概率序列。</li><li>Proposal Evaluation Module(PEM)：<ol><li>[[PapersRead#BMN#BM layer|BM layer]]</li><li>通过conv3d, conv2d 得到置信度图。</li></ol></li><li>生成结果：<ol><li>把两条边界概率序列中大于<script type="math/tex">极大值\times  \frac{1}{2}</script> 或是峰值（极大值）的都看作开始或结束边界.</li><li><script type="math/tex">n^2</script>复杂度两两组合，获得一系列proposals:</li><li>根据置信度图获得各个proposals的置信度。<ol><li>the proposal denoted: <script type="math/tex">φ = (t_s, t_e, p^s_{ts} , p^e_{te} , p_{cc}, p_{cr})</script><br>where <script type="math/tex">p^s_{ts},  p^e_{te}</script> are starting and ending probabilities.  <script type="math/tex">p_{cc}, p_{cr}</script> are classification confidence and regression confidence score.</li><li>get final score: <script type="math/tex">p_f = p^s_{ts} · p^e_{te} · \sqrt{p_{cc}· p_{cr})}</script></li></ol></li><li>利用Soft-NMS去冗余。</li></ol></li></ol><h2 id="置信度图："><a href="#置信度图：" class="headerlink" title="置信度图："></a>置信度图：</h2><p><img src="/imgs/Pasted image 20230807202541.png" alt></p><ol><li><script type="math/tex; mode=display">M_C\in R^{D×T}</script></li><li>由开始点和长度决定结束点，从而确定一个proposal. 所以上图对应所有任意视频段的置信度。</li><li>duration dim: proposal长度.</li><li>starting dim: 开始点位置。</li><li>同一行对应的proposals对应相同的长度。同一列队对应的proposals拥有相同的开始点。同一负对角线对应的proposals拥有相同的结束边界。右下角部分proposals超出视频范围，无意义。</li></ol><h2 id="BM-layer"><a href="#BM-layer" class="headerlink" title="BM layer"></a>BM layer</h2><ol><li>The goal: uniformly sample N points in <script type="math/tex">S_{F} ∈ R^{C×T}</script>  between starting boundary <script type="math/tex">t_{s}</script> and ending boundary <script type="math/tex">t_{e}</script> of each proposal <script type="math/tex">φ_{i,j}</script>, and get proposal feature <script type="math/tex">m^f_{i,j} ∈ R^{C×N}</script> with rich context (actually sampling in [<script type="math/tex">t_S-0.25d, t_e+0.25d</script>]).<br><img src="/imgs/Pasted image 20230810081026.png" alt></li><li>two problems:<ol><li>how to sample feature in non-integer point:<br><img src="/imgs/Pasted image 20230808205131.png" alt="Pasted image 20230808205131"></li><li>how to sample feature for all proposals simultaneously:<ol><li>expanding <script type="math/tex">w_{i,j} ∈ R^{N ×T}</script>  to  <script type="math/tex">W ∈ R^{N ×T ×D×T}</script> for all proposals in BM confidence map.</li><li>get <script type="math/tex">M_F ∈ R^{C×N×D×T}</script> by using dot product:   <script type="math/tex">S_{F} ∈ R^{C×T}</script> and  <script type="math/tex">W^T ∈ R^{T×N×D×T}</script>. （<em><script type="math/tex">W</script> can be pre-generated because it’s the same for different videos, the inference speed of BM layer is very fast.</em> Is T is the same for the different videos? Ans: [[#BMN#Base module|Base module]] and [[#BMN#Training of BMN#Training Data Construction|Training Data Construction]]. TODO: review code）</li></ol></li></ol></li></ol><h2 id="Base-module"><a href="#Base-module" class="headerlink" title="Base module"></a>Base module</h2><ol><li><em>adopt a long observation window with length <script type="math/tex">l_ω</script> to truncate the untrimmed feature sequence with length <script type="math/tex">l_f</script> .</em></li><li>So here <script type="math/tex">l_w</script> is <script type="math/tex">T</script> in <script type="math/tex">S_{F} ∈ R^{C×T}</script>.</li></ol><h2 id="Proposal-Evaluation-Module-PEM"><a href="#Proposal-Evaluation-Module-PEM" class="headerlink" title="Proposal Evaluation Module(PEM)"></a>Proposal Evaluation Module(PEM)</h2><ol><li>Final generate: <script type="math/tex">M_C\in R^{D×T}</script>, but there are two predicted <script type="math/tex">M_C</script>: <script type="math/tex">M_{CC}</script>, <script type="math/tex">M_{CR}</script>, being trained using binary classification and regression loss function separately. TODO: review code.</li></ol><h2 id="Training-of-BMN"><a href="#Training-of-BMN" class="headerlink" title="Training of BMN"></a>Training of BMN</h2><h3 id="TEM-vs-PEM"><a href="#TEM-vs-PEM" class="headerlink" title="TEM vs PEM:"></a>TEM vs PEM:</h3><ol><li>TEM: learns local boundary.</li><li>PEM: pattern global proposal context.</li></ol><h3 id="Training-Data-Construction"><a href="#Training-Data-Construction" class="headerlink" title="Training Data Construction:"></a>Training Data Construction:</h3><ol><li>firstly, extract all feature sequence F with length.</li><li>get many observation windows with length <script type="math/tex">l_w</script> with 50% overlap.</li><li>here every window contains at least one ground-truth action instance.</li></ol><h3 id="Label-Assignment"><a href="#Label-Assignment" class="headerlink" title="Label Assignment"></a>Label Assignment</h3><h4 id="TEM"><a href="#TEM" class="headerlink" title="TEM"></a>TEM</h4><ol><li>denote its starting and ending regions as <script type="math/tex">r_S = [t_s − d_g /10, t_s +d_g/10]</script> and <script type="math/tex">r_E =[t_e−d_g/10,t_e+d_g/10]</script>separately.</li><li>denote its local region as <script type="math/tex">r_{t_n} = [t_n −d_f /2, t_n +d_f /2]</script>, where <script type="math/tex">d_f = t_n −t_{n−1}</script> is the temporal interval between two locations.</li><li>Then calculate overlap ratio IoR of <script type="math/tex">r_{t_n}</script> with <script type="math/tex">r_S</script> and <script type="math/tex">r_E</script> separately, and denote maximum IoR as <script type="math/tex">g^s_{t_n}</script> and  <script type="math/tex">g^e_{t_n}</script> separately.</li><li><em>here IoR is defined as the overlap ratio with ground-truth proportional to the duration of this region.</em> TODO: code review.</li><li>Thus generate <script type="math/tex">G_{S,ω}=\{g^s_{t_n}\}^{l_w}_{n=1}</script> and <script type="math/tex">G_{E,ω}=\{g^e_{t_n}\}^{l_w}_{n=1}</script>  as label of TEM.</li></ol><h4 id="PEM"><a href="#PEM" class="headerlink" title="PEM"></a>PEM</h4><ol><li>Purpose: BM label map <script type="math/tex">G_C ∈ R^{D×T}</script>.</li><li>For a proposal <script type="math/tex">φ_{i,j}=(t_s=t_j, t_e=t_j+t_i)</script> , calculate its IoU with all <script type="math/tex">φ_g</script> in <script type="math/tex">Ψ_ω</script>, and denote the maximum IoU as <script type="math/tex">g^c_{i,j}</script> . Thus we can generate <script type="math/tex">G_C=\{g^c_{i,j}\}^{D,l_ω}_{i,j=1}</script>  as label of PEM.</li></ol><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Loss-of-TEM"><a href="#Loss-of-TEM" class="headerlink" title="Loss of TEM"></a>Loss of TEM</h4><ol><li>adopt weighted binary logistic regression loss function <script type="math/tex">L_{bl}</script>,  to get the sum of starting and ending losses:<script type="math/tex; mode=display">L_{TEM} =L_{bl}(P_S,G_S)+L_{bl}(P_E,G_E)</script></li><li><script type="math/tex; mode=display">L_{bl}(P,G): \frac{1}{l_w}\sum_{i=1}^{l_w}(a^+·b_i·log(p_i)+a^-·(1-b_i)·log(1-p_i))</script>where <script type="math/tex">b_i = sign(g_i − θ)</script> is a two-value function used to convert <script type="math/tex">g_i</script> from [0, 1] to {0, 1}0, 1} based on overlap threshold<script type="math/tex">θ = 0.5</script>. Denoting <script type="math/tex">l^+=\sum b_i</script>  and <script type="math/tex">l^− = l_ω −l^+</script>, the weighted terms are <script type="math/tex">α^+ = \frac{l_w}{l^+}</script> and <script type="math/tex">α^- = \frac{l_w}{l^-}</script>.</li></ol><h4 id="Loss-of-PEM"><a href="#Loss-of-PEM" class="headerlink" title="Loss of PEM"></a>Loss of PEM</h4><ol><li>Define:<script type="math/tex; mode=display">L_{PEM} =L_C(M_{CC},G_C)+λ·L_R(M_{CR},G_C)</script></li><li>here  <script type="math/tex">L_{bl}</script>  for <script type="math/tex">L_C</script>  ,  L2 loss for <script type="math/tex">L_R</script> .  <script type="math/tex">λ = 10</script> .</li><li>to balance the ratio between positive and negative samples in <script type="math/tex">L_R</script> , take all points with <script type="math/tex">g^C_{i,j}>0.6</script>  as positive, and randomly sample  <script type="math/tex">g^C_{i,j}<0.2</script>  as negative, ensure 1:1 for positive: negative.</li></ol><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><script type="math/tex; mode=display">\begin{aligned}L=L_{LEM} +λ_1 ·L_{GEM} +λ_2 ·L_2(Θ)\end{aligned}</script><p>where <script type="math/tex">L_2(Θ)</script> is L2 regularization term,  <script type="math/tex">λ_1</script>, <script type="math/tex">λ_2</script> are set to 1, 0.000 to ensure different modules are trained evenly.</p><p>Refs:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/337432552" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/337432552</a></li><li><a href="https://arxiv.org/pdf/1907.09702.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.09702.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BMN(Boundary-Matching Network) 详解。&lt;/p&gt;
    
    </summary>
    
      <category term="ActionDetection" scheme="https://huzhenyou.github.io/categories/ActionDetection/"/>
    
    
      <category term="Action Detection" scheme="https://huzhenyou.github.io/tags/Action-Detection/"/>
    
      <category term="Proposal Generation" scheme="https://huzhenyou.github.io/tags/Proposal-Generation/"/>
    
      <category term="ActivityNet Challenge 2019" scheme="https://huzhenyou.github.io/tags/ActivityNet-Challenge-2019/"/>
    
      <category term="DeepLearning" scheme="https://huzhenyou.github.io/tags/DeepLearning/"/>
    
  </entry>
  
</feed>
